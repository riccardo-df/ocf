% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ordered-ml.R
\name{ordered_ml}
\alias{ordered_ml}
\title{Ordered Machine Learning}
\usage{
ordered_ml(y = NULL, X = NULL, learner = "forest", scale = TRUE)
}
\arguments{
\item{y}{Outcome vector.}

\item{X}{Covariate matrix (no intercept).}

\item{learner}{String, either \code{"forest"} or \code{"l1"}. Selects the base learner to estimate each expectation.}

\item{scale}{Logical, whether to scale the covariates. Ignored if \code{learner} is not \code{"l1"}.}
}
\value{
Object of class \code{oml}.
}
\description{
Estimation strategy to estimate conditional choice probabilities for ordered non-numeric outcomes.
}
\details{
Ordered machine learning expresses conditional choice probabilities as the difference between the cumulative probabilities 
of two adjacent classes, which in turn can be expressed as conditional expectations of binary variables:

\deqn{p_m \left( X_i \right) = \mathbb{E} \left[ 1 \left( Y_i \leq m \right) | X_i \right] - \mathbb{E} \left[ 1 \left( Y_i \leq m - 1 \right) | X_i \right]}

Then we can separately estimate each expectation using any regression algorithm and pick the difference between the m-th and the
(m-1)-th estimated surfaces to estimate conditional probabilities.\cr

\code{\link{ordered_ml}} combines this strategy with either regression forests or penalized logistic regression with an L1 penalty,
according to the user-specified parameter \code{learner}.\cr

If \code{learner == "forest"}, then the \code{\link[orf]{orf}}
function is called from an external package, as this estimator has already been proposed by Lechner and Okasa (2019).\cr

If \code{learner == "l1"}, 
the penalty parameters are chosen via 10-fold cross-validation and \code{\link[stats]{model.matrix}} is used to handle non-numeric covariates. 
Additionally, if \code{scale == TRUE}, the covariates are scaled to have zero mean and unit variance.
}
\examples{
\donttest{## Generate synthetic data.
set.seed(1986)

data <- generate_ordered_data(100)
sample <- data$sample
Y <- sample$Y
X <- sample[, -1]

## Training-test split.
train_idx <- sample(seq_len(length(Y)), floor(length(Y) * 0.5))

Y_tr <- Y[train_idx]
X_tr <- X[train_idx, ]

Y_test <- Y[-train_idx]
X_test <- X[-train_idx, ]

## Fit ordered machine learning on training sample using two different learners.
ordered_forest <- ordered_ml(Y_tr, X_tr, learner = "forest")
ordered_l1 <- ordered_ml(Y_tr, X_tr, learner = "l1")

## Predict out of sample.
predictions_forest <- predict(ordered_forest, X_test)
predictions_l1 <- predict(ordered_l1, X_test)

## Compare predictions.
cbind(head(predictions_forest), head(predictions_l1))}

}
\references{
\itemize{
  \item Di Francesco, R. (2023). Ordered Correlation Forest. arXiv preprint \href{https://arxiv.org/abs/2309.08755}{arXiv:2309.08755}.
}
}
\seealso{
\code{\link{multinomial_ml}}, \code{\link{ocf}}
}
\author{
Riccardo Di Francesco
}
