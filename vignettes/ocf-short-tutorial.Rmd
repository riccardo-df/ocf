---
title: "Short Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Short Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE, eval = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(ocf)
```

In this tutorial, we show how to use the `ocf` package to estimate and make inference about the conditional choice probabilities and the covariates' marginal effects. 

Before diving in the coding, we provide an overview of the statistical problem at hand.
## Ordered Choice Models
We postulate the existence of a latent and continuous outcome variable $Y_i^*$, assumed to obey the following regression model:

$$ Y_i^* = g \left( W_i \right) + \epsilon_i  $$
where $W_i$ consists of a set of raw covariates, $g \left( \cdot \right)$ is a potentially non-linear regression function, and $\epsilon_i$ is independent of $W_i$. 

Then, an observational rule links the observed outcome $Y_i$ to the latent outcome $Y_{i}^*$ using unknown threshold parameters $- \infty = \zeta_0 < \zeta_1 < \dots < \zeta_{M - 1} < \zeta_M = \infty$ that define intervals on the support of $Y_i^*$, with each interval corresponding to one of the $M$ categories or classes of $Y_i$:

$$ \zeta_{m - 1} < Y_i^* \leq \zeta_{m} \implies Y_i = m, \quad m = 1, \dots, M $$

The statistical targets of interest are the conditional choice probabilities:

$$ p_m \left( W_i \right) := \mathbb{P} \left( Y_i = m | W_i \right) $$

and the marginal effect of the $j$-th covariate on $p_m \left( \cdot \right)$:

$$
\nabla^j p_m \left( w \right) := 
  \begin{cases}
    \frac{\partial p_m \left( w \right)}{\partial w_j}, & \text{if } w_j \text{ is continuous}  \\
    p_m \left( \lceil w_j \rceil \right) - p_m \left( \lfloor w_j \rfloor \right), & \text{if } w_j \text{ is discrete}
  \end{cases}
$$
where $w_j$ is the $j$-th element of the vector $w$ and $\lceil w_j \rceil$ and $\lfloor w_j \rfloor$ correspond to $w$ with its $j$-th element rounded up and down to the closest integer.

## Code
For illustration purposes, we generate a synthetic data set. Details about the employed DGP can be retrieved by running `help(generate_ordered_data)`.

```{r data-generation, eval = TRUE}
## Generate synthetic data.
set.seed(1986)

n <- 1000
data <- generate_ordered_data(n)

sample <- data$sample
Y <- sample$Y
X <- sample[, -1]

table(Y)

head(X)
```

### Conditional Probabilities
The `ocf` function constructs a collection of forests, one for each category of `y` (three in this case). We can then use the forests to predict out-of-sample conditional probabilities using the `predict` method. By default, `predict` returns a matrix with the predicted probabilities and a vector of predicted class labels (each observation is labelled to the highest-probability class). 

```{r adaptive-ocf, eval = FALSE}
## Training-test split.
train_idx <- sample(seq_len(length(y)), floor(length(y) * 0.5))

y_tr <- y[train_idx]
X_tr <- X[train_idx, ]

y_test <- y[-train_idx]
X_test <- X[-train_idx, ]

## Fit ocf on training sample. Use default settings.
forests <- ocf(y_tr, X_tr)

## Summary of data and tuning parameters.
summary(forests)

## Out-of-sample predictions.
predictions <- predict(forests, X_test)

head(predictions$probabilities)
table(y_test, predictions$classification)
```

We can also implement honesty, which is a necessary condition to produce asymptotically normal and consistent predictions. In the following, we set `honesty = TRUE` to construct honest forests.

```{r honest-ocf, eval = FALSE}
## Honest forests.
honest_forests <- ocf(y_tr, X_tr, honesty = TRUE)
honest_predictions <- predict(honest_forests, X_test)

## Compare predictions with adaptive fit.
cbind(head(predictions$probabilities), head(honest_predictions$probabilities))
```

To estimate standard errors for the predicted probabilities, we set `inference = TRUE`. This requires also to set `honesty = TRUE`: the formula for the variance is valid only for honest predictions. The estimation of standard errors considerably slows down the routine. However, we can increase the number of threads used to construct the forests to speed up the routine.

```{r honest-ocf-inference, eval = FALSE}
## Compute standard errors.
honest_forests <- ocf(y_tr, X_tr, honesty = TRUE, inference = TRUE, n.threads = 0) # Use all CPUs.
head(honest_forests$predictions$standard.errors)
```

### Covariates' Marginal Effects
The `marginal_effects` function post-processes the predictions to estimate mean marginal effects, marginal effects at the mean, or marginal effects at the median, according to the `eval` argument. In the following, we construct our forests in the training sample and use them to estimate the marginal effects at the mean in the test sample.

```{r adaptive-me, eval = FALSE}
## Fit ocf on training sample.
forests <- ocf(y_tr, X_tr)

## Marginal effects at the mean on test sample.
me_atmean <- marginal_effects(forests, data = X_test, eval = "atmean")
summary(me_atmean)
```

As before, we can set `inference = TRUE` to estimate the standard errors. Again, this requires the use of honest forests and considerably slows down the routine.

```{r honest-me, eval = FALSE}
## Honest forests.
honest_forests <- ocf(y_tr, X_tr, honesty = TRUE) # Notice we do not need inference here!

## Compute standard errors.
honest_me_atmean <- marginal_effects(honest_forests, data = X_test , eval = "atmean", inference = TRUE)

## LATEX.
print(honest_me_atmean, latex = TRUE)
```
